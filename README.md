# Big Data Cluster: MinIO + Spark + Airflow

A self-contained local development stack for building and testing financial fraud detection pipelines using Apache Spark, MinIO (S3-compatible storage), and Apache Airflow for orchestration.

## Quick Start

### 1. Start the full local stack

```powershell
./start-services.sh
```

Or manually with Docker Compose:

```powershell
docker-compose up -d --build
docker-compose ps
```

### 2. Access the Services

| Service             | URL                   | Credentials                 |
| ------------------- | --------------------- | --------------------------- |
| **Airflow UI**      | http://localhost:8080 | `admin` / `admin`           |
| **MinIO Console**   | http://localhost:9001 | `minioadmin` / `minioadmin` |
| **Spark Master UI** | http://localhost:8088 | -                           |
| **Spark Worker UI** | http://localhost:8089 | -                           |

### 3. Run the Fraud Detection Pipeline

1. Open **Airflow UI** at http://localhost:8080
2. Find the DAG: **`fintech_fraud_pipeline_v4`**
3. Toggle it **ON**
4. Click the **Play** button to trigger manually or wait for scheduled run (`@daily`)

## Project Structure

```
├── docker-compose.yml           # Main service orchestration
├── Dockerfile                   # Placeholder (not used)
├── airflow/                     # Airflow image & connections setup
│   ├── Dockerfile
│   ├── load_connections.sh
│   └── requirements.txt
├── dags/                        # Airflow DAGs
│   └── fraud_pipeline_dag.py    # Main fraud detection pipeline
├── spark/                       # Spark image context
│   └── Dockerfile
├── spark-jobs/                  # PySpark job scripts
│   └── process_fraud_data.py    # ETL: reads MinIO → transforms → writes to Snowflake
├── include/                     # Helper scripts & data
│   └── data/paysim/raw/         # PaySim CSV files (place your data here)
├── tests/                       # Integration tests
│   └── test_smoke_workflow.py
├── logs/                        # Airflow & Spark logs (created at runtime)
└── plugins/                     # Airflow plugins directory
```

## How It Works

### Architecture

```
┌──────────────────────┐
│   Airflow Scheduler  │ (orchestrates)
└──────────────────────┘
          ↓
┌──────────────────────────────────────────────┐
│  Airflow DAG: fintech_fraud_pipeline_v4      │
│  ┌──────────────────────────────────────────┐│
│  │ Task 1: ingest_to_minio                  ││ Upload PaySim CSV to MinIO
│  └──────────────────────────────────────────┘│
│                    ↓                          │
│  ┌──────────────────────────────────────────┐│
│  │ Task 2: process_with_spark               ││ Spark processes data & loads to Snowflake
│  └──────────────────────────────────────────┘│
└──────────────────────────────────────────────┘
          ↓                    ↓                   ↓
    [MinIO Storage]    [Spark Executors]   [Snowflake DW]
```

### Pipeline Flow

1. **Ingest**: Python operator uploads PaySim CSV from `/opt/airflow/include/data/paysim/raw/` to MinIO bucket `raw-data`
2. **Process**: Spark job reads from MinIO, transforms the data (adds transaction ID, timestamps, type conversions), and writes to Snowflake table `FACT_TRANSACTIONS`

## Setup & Configuration

### Environment Variables

The `.env` file controls all configuration. Key variables:

```env
# MinIO
MINIO_PORT=9000
MINIO_ROOT_USER=minioadmin
MINIO_ROOT_PASSWORD=minioadmin
MINIO_HOST=minio

# Spark
SPARK_MASTER_HOST=spark-master
SPARK_MASTER_PORT=7077
SPARK_MASTER_WEBUI_PORT=8088
SPARK_WORKER_CORES=2
SPARK_WORKER_MEMORY=2g

# Airflow
AIRFLOW__CORE__EXECUTOR=LocalExecutor
AIRFLOW__WEBSERVER__WORKERS=2
AIRFLOW_FERNET_KEY=<auto-generated>

# Snowflake (optional, for full pipeline)
SNOWFLAKE_USER=your_user
SNOWFLAKE_PASSWORD=your_password
SNOWFLAKE_ACCOUNT=your_account
SNOWFLAKE_DB=FRAUD_DETECTION
SNOWFLAKE_WAREHOUSE=COMPUTE_WH
```

### Data Setup

Place your PaySim CSV files in: `include/data/paysim/raw/`

Naming convention:

- `paysim_day1.csv`, `paysim_day2.csv`, ..., `paysim_day25.csv`
- Or upload manually via MinIO Console at http://localhost:9001

## Service Management

### Start Stack

```powershell
./start-services.sh
# or
docker-compose up -d --build
```

### View Logs

```powershell
# Airflow Scheduler logs
docker-compose logs -f airflow-scheduler

# Spark Master logs
docker-compose logs -f spark-master

# View all logs
docker-compose logs -f
```

### Scale Spark Workers

```powershell
docker-compose up -d --scale spark-worker=3
```

### Stop Stack

```powershell
./stop-services.sh
# or
docker-compose down -v
```

## Common Tasks

### Upload PaySim Data to MinIO

From inside the Airflow container:

```powershell
docker exec -it airflow-webserver bash
python /opt/airflow/include/upload_test_data.py
```

Or use MinIO Console: http://localhost:9001

### Monitor Spark Jobs

1. Go to **Spark Master UI**: http://localhost:8088
2. Click on running or completed applications to see logs
3. View executor details and stage breakdowns

### Check Airflow DAG Logs

1. Open **Airflow UI**: http://localhost:8080
2. Click on the DAG run
3. Click on a task to view detailed logs

### Run Integration Tests

```powershell
python -m pytest tests/test_smoke_workflow.py::test_smoke_pipeline_completes -v
```

## Troubleshooting

### Airflow Can't Connect to MinIO or Spark

Ensure all services are on the same Docker network:

```powershell
docker network inspect spark-minio-network
```

Services should communicate via container names (e.g., `minio:9000`, `spark-master:7077`).

### Spark S3A Timeout or Connection Issues

The DAG already includes fixes for common S3A issues:

- Numeric socket timeout configs (milliseconds)
- S3A path-style access enabled
- Connection retry settings

If you still see errors, check Spark logs:

```powershell
docker logs spark-master
docker logs spark-worker
```

### Airflow DAG Not Appearing

The `airflow-init` service must complete successfully:

```powershell
docker-compose logs airflow-init
```

Then restart the scheduler:

```powershell
docker-compose restart airflow-scheduler
```

### MinIO Bucket Access Denied

Verify credentials in `.env` file match those passed to Spark jobs. Both must use same `MINIO_ROOT_USER` and `MINIO_ROOT_PASSWORD`.

## Prerequisites

- **Docker** (Desktop or Engine)
- **Docker Compose** (v1.29+ or v2)
- **Python 3.8+** (for running tests locally, optional)

## Development Notes

- All services run on a dedicated Docker network (`spark-minio-network`) for easy internal communication
- Airflow uses PostgreSQL as its metadata database
- Spark runs with configurable worker count via `--scale` flag
- MinIO data persists in Docker volume `minio-data`

## Next Steps

1. **Add more DAGs**: Create additional Airflow DAGs in `dags/`
2. **Scale workers**: Use `--scale spark-worker=N` to add more compute capacity
3. **Connect other data sources**: Modify DAGs to ingest from additional sources
4. **Set up alerts**: Configure Airflow email notifications for failures
5. **Monitor metrics**: Add Prometheus & Grafana for cluster monitoring

## Support

For issues or questions, check logs and ensure all services are healthy:

```powershell
docker-compose ps
```

All services should show `Up` status with no errors.
