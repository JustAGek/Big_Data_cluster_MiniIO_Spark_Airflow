version: "3.8"

services:
  # MinIO Object Storage
  minio:
    image: minio/minio:latest
    container_name: minio
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}
    command: server /data --console-address ":9001"
    ports:
      - "${MINIO_PORT}:${MINIO_PORT}"
      - "9001:9001"
    volumes:
      - minio-data:/data
    networks:
      - spark-minio-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:${MINIO_PORT}/minio/health/live"]
      interval: 30s
      timeout: 20s
      retries: 3

  minio-init:
    image: minio/mc:latest
    container_name: minio-init
    depends_on:
      minio:
        condition: service_healthy
    entrypoint: >
      /bin/sh -c "
      mc alias set minio http://${MINIO_HOST}:${MINIO_PORT} ${MINIO_ROOT_USER} ${MINIO_ROOT_PASSWORD};
      mc mb minio/input-bucket --ignore-existing;
      mc mb minio/output-bucket --ignore-existing;
      mc mb minio/raw-data --ignore-existing;
      mc mb minio/processed-data --ignore-existing;
      echo 'Buckets created successfully';
      "
    networks:
      - spark-minio-network

  # Spark Master
  spark-master:
    build: ./spark
    container_name: spark-master
    command:
      [
        "/opt/spark/bin/spark-class",
        "org.apache.spark.deploy.master.Master",
        "--host",
        "${SPARK_MASTER_HOST}",
        "--port",
        "${SPARK_MASTER_PORT}",
        "--webui-port",
        "${SPARK_MASTER_WEBUI_PORT}",
      ]
    environment:
      SPARK_MASTER_HOST: ${SPARK_MASTER_HOST}
      SPARK_MASTER_PORT: ${SPARK_MASTER_PORT}
      SPARK_MASTER_WEBUI_PORT: ${SPARK_MASTER_WEBUI_PORT}
      SPARK_NO_DAEMONIZE: true
    ports:
      - "${SPARK_MASTER_PORT}:${SPARK_MASTER_PORT}"
      - "${SPARK_MASTER_WEBUI_PORT}:${SPARK_MASTER_WEBUI_PORT}"
    volumes:
      - ./spark-jobs:/opt/spark-jobs
      - spark-logs:/opt/spark/logs
    networks:
      - spark-minio-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:${SPARK_MASTER_WEBUI_PORT}"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Spark Worker
  spark-worker:
    build: ./spark
    container_name: spark-worker
    command:
      [
        "/opt/spark/bin/spark-class",
        "org.apache.spark.deploy.worker.Worker",
        "spark://${SPARK_MASTER_HOST}:${SPARK_MASTER_PORT}",
      ]
    depends_on:
      spark-master:
        condition: service_healthy
    environment:
      SPARK_WORKER_CORES: ${SPARK_WORKER_CORES}
      SPARK_WORKER_MEMORY: ${SPARK_WORKER_MEMORY}
      SPARK_WORKER_PORT: ${SPARK_WORKER_PORT}
      SPARK_WORKER_WEBUI_PORT: ${SPARK_WORKER_WEBUI_PORT}
      SPARK_NO_DAEMONIZE: true
    ports:
      - "${SPARK_WORKER_WEBUI_PORT}:${SPARK_WORKER_WEBUI_PORT}"
    volumes:
      - ./spark-jobs:/opt/spark-jobs
      - spark-logs:/opt/spark/logs
    networks:
      - spark-minio-network

  # Postgres for Airflow
  postgres:
    image: postgres:13
    container_name: airflow-postgres
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
    volumes:
      - pgdata:/var/lib/postgresql/data
    networks:
      - spark-minio-network

  # Airflow Init
  airflow-init:
    build: ./airflow
    container_name: airflow-init
    restart: "no"
    depends_on:
      postgres:
        condition: service_started
      minio:
        condition: service_healthy
      minio-init:
        condition: service_completed_successfully
    environment:
      MINIO_HOST: ${MINIO_HOST}
      MINIO_PORT: ${MINIO_PORT}
      MINIO_LOGIN: ${MINIO_ROOT_USER}
      MINIO_PASSWORD: ${MINIO_ROOT_PASSWORD}
      IMPORT_CONN_JSON: "false"
      AIRFLOW__CORE__EXECUTOR: ${AIRFLOW__CORE__EXECUTOR}
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW_FERNET_KEY}
      # Connections constructed from .env variables
      # FIX: Used URL Encoding for spark:// prefix so Airflow passes it correctly to SparkSubmitOperator
      AIRFLOW_CONN_SPARK_DEFAULT: 'spark://spark%3A%2F%2F${SPARK_MASTER_HOST}:${SPARK_MASTER_PORT}'
      AIRFLOW_CONN_MINIO_DEFAULT: 'aws://${MINIO_ROOT_USER}:${MINIO_ROOT_PASSWORD}@?extra__aws__endpoint_url=http%3A%2F%2F${MINIO_HOST}%3A${MINIO_PORT}'
      AIRFLOW_CONN_SNOWFLAKE_DEFAULT: 'snowflake://${SNOWFLAKE_USER}:${SNOWFLAKE_PASSWORD}@${SNOWFLAKE_ACCOUNT}/${SNOWFLAKE_DB}/${SNOWFLAKE_SCHEMA}?warehouse=${SNOWFLAKE_WAREHOUSE}&role=${SNOWFLAKE_ROLE}'
    entrypoint:
      [
        "/bin/bash",
        "-c",
        "airflow db upgrade || true; \ \
         airflow users create --username admin --firstname Admin --lastname User --role Admin --email admin@example.com --password admin || true; \ \
         /opt/airflow/load_connections.sh || true",
      ]
    volumes:
      - ./dags:/opt/airflow/dags
      - ./include:/opt/airflow/include
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
      - /var/run/docker.sock:/var/run/docker.sock
      - ./spark-jobs:/opt/spark-jobs
    networks:
      - spark-minio-network

  # Airflow Webserver
  airflow-webserver:
    build: ./airflow
    container_name: airflow-webserver
    restart: always
    depends_on:
      - postgres
      - airflow-init
    environment:
      AIRFLOW__CORE__EXECUTOR: ${AIRFLOW__CORE__EXECUTOR}
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW_FERNET_KEY}
      AIRFLOW__API__AUTH_BACKENDS: airflow.api.auth.backend.basic_auth
      AIRFLOW__WEBSERVER__WORKERS: ${AIRFLOW__WEBSERVER__WORKERS}
      # Connections
      AIRFLOW_CONN_SPARK_DEFAULT: 'spark://spark%3A%2F%2F${SPARK_MASTER_HOST}:${SPARK_MASTER_PORT}'
      AIRFLOW_CONN_MINIO_DEFAULT: 'aws://${MINIO_ROOT_USER}:${MINIO_ROOT_PASSWORD}@?extra__aws__endpoint_url=http%3A%2F%2F${MINIO_HOST}%3A${MINIO_PORT}'
      AIRFLOW_CONN_SNOWFLAKE_DEFAULT: 'snowflake://${SNOWFLAKE_USER}:${SNOWFLAKE_PASSWORD}@${SNOWFLAKE_ACCOUNT}/${SNOWFLAKE_DB}/${SNOWFLAKE_SCHEMA}?warehouse=${SNOWFLAKE_WAREHOUSE}&role=${SNOWFLAKE_ROLE}'
    ports:
      - "8080:8080"
    volumes:
      - ./dags:/opt/airflow/dags
      - ./include:/opt/airflow/include
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
      - /var/run/docker.sock:/var/run/docker.sock
      - ./spark-jobs:/opt/spark-jobs
    networks:
      - spark-minio-network
    command: webserver

  # Airflow Scheduler
  airflow-scheduler:
    build: ./airflow
    container_name: airflow-scheduler
    restart: always
    depends_on:
      - postgres
      - airflow-init
    environment:
      AIRFLOW__CORE__EXECUTOR: ${AIRFLOW__CORE__EXECUTOR}
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}
      AIRFLOW__API__AUTH_BACKENDS: airflow.api.auth.backend.basic_auth
      # Connections
      AIRFLOW_CONN_SPARK_DEFAULT: 'spark://spark%3A%2F%2F${SPARK_MASTER_HOST}:${SPARK_MASTER_PORT}'
      AIRFLOW_CONN_MINIO_DEFAULT: 'aws://${MINIO_ROOT_USER}:${MINIO_ROOT_PASSWORD}@?extra__aws__endpoint_url=http%3A%2F%2F${MINIO_HOST}%3A${MINIO_PORT}'
      AIRFLOW_CONN_SNOWFLAKE_DEFAULT: 'snowflake://${SNOWFLAKE_USER}:${SNOWFLAKE_PASSWORD}@${SNOWFLAKE_ACCOUNT}/${SNOWFLAKE_DB}/${SNOWFLAKE_SCHEMA}?warehouse=${SNOWFLAKE_WAREHOUSE}&role=${SNOWFLAKE_ROLE}'
    volumes:
      - ./dags:/opt/airflow/dags
      - ./include:/opt/airflow/include
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
      - /var/run/docker.sock:/var/run/docker.sock
      - ./spark-jobs:/opt/spark-jobs
    networks:
      - spark-minio-network
    command: scheduler

volumes:
  minio-data:
  spark-logs:
  pgdata:

networks:
  spark-minio-network:
    driver: bridge
    name: spark-minio-network